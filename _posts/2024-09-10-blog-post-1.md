---
title: 'Stable Diffusion Mathmatics + Updates on Projects'
date: 2024-09-10
permalink: /posts/2024/09/blog-post-1/
tags:
  - Stable Diffusion
  - Swift
  - ML
---

## Introduction
I completed these notes on the mathematics behind Stable Diffusion a few months ago. However, I wasnâ€™t able to turn it into a full project due to the large dataset requirements and the complexity of developing unique code

## Forward Diffusion Process

The forward diffusion process is modeled as:

$$
x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_t
$$

Where:
- \( x_t \) represents the data at time step \( t \),
- \( \alpha_t \) is the noise schedule parameter,
- \( \epsilon_t \) is Gaussian noise drawn from \( \mathcal{N}(0, I) \).

## Reverse Diffusion Process

The reverse process is modeled by a distribution \( p_\theta(x_{t-1} | x_t) \) with learned mean and variance:

$$
p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
$$

The goal is to train the neural network to predict the reverse transitions by optimizing the loss function.

## Loss Function

The training loss is based on the Evidence Lower Bound (ELBO):

$$
\mathcal{L}_{\text{ELBO}} = \mathbb{E}_q \left[ \log \frac{q(x_{1:T}|x_0)}{p_\theta(x_{0:T})} \right]
$$

Where \( q(x_{1:T} | x_0) \) represents the forward process and \( p_\theta(x_{0:T}) \) represents the learned reverse process.

The variational lower bound consists of a KL-divergence term that regularizes the learned distribution to match the true posterior.

## Reparameterization Trick

The reparameterization trick is used to optimize the latent variables:

$$
z = \mu + \sigma \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, 1)
$$

This allows for gradient-based optimization of the variational autoencoder (VAE) loss.

## Evidence Lower Bound (ELBO)

The ELBO is expressed as:

$$
\mathcal{L}_{\text{ELBO}} = \mathbb{E}_q \left[ \log p_\theta(x_0 | z) \right] - \text{KL}(q(z|x_0) \| p(z))
$$

Where \( p_\theta(x_0 | z) \) is the reconstruction term and \( \text{KL}(\cdot \| \cdot) \) is the Kullback-Leibler divergence between the approximate posterior and the prior.

## Current Work
Currently, I am working on a Swift app and diving deeper into [Deep Reinforcement Learning](https://rail.eecs.berkeley.edu/deeprlcourse/).