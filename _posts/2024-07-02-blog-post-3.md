---
title: 'Completed on data capturing for HandSignalCV'
date: 2024-07-02
permalink: /posts/2024/07/blog-post-3/
tags:
  - HandSignalCV
  - hdf5
  - mediapipe
---

## New Learning of the day
As ususal, while I was learning more concepts of Machine learning, I happen to cross multiple concepts that I would like to share in my blog as it can potentially be used on other projects. You can skip this part if you are interested on my HandSignalCV project.

### Numerical Approximation of Gradients (Gradient Checking Algorithm)

Gradient checking algorithms are used primarily for debugging and have similar uses as gradient descent. However, gradient checking algorithms do not work with dropout; thus, dropout needs to be turned off during gradient checking and turned back on afterward.

Another gradient descent algorithm is the momentum algorithm, which works faster than the standard algorithm. Additionally, RMSprop (Root Mean Square Propagation) also speeds up the gradient descent process.

For mini-batch algorithms, learning rate decay helps in getting closer to the optimum point.

### Hyperparameters

Another important concept is hyperparameters. Deciding on hyperparameters is essential to tune the model for better results. Here is a useful resource on [hyperparameter tuning and optimization](https://medium.com/optimizing-hyperparameters/hyperparameter-tuning-optimization-eb8391d8674).

## Hand Signal Detection Project

Returning to my hand signal detection project, I found an interesting [implementation](https://github.com/rrupeshh/Simple-Sign-Language-Detector/blob/master/capture.py) by someone else. This person uses the [cv2.createTrackbar](https://www.geeksforgeeks.org/python-opencv-gettrackbarpos-function/) feature to adjust color features for data augmentation. I can implement color changes and display landmarks, which will provide the best results for data augmentation.

Additionally, when capturing data, I can use the following code to save images to a folder:

```python
if # of captures exceed certain limits:
    cv2.imwrite(f'{folder}/Image_{time.time()}.jpg', frameWhite) # Change the folder to training set
else:
    # Vice versa
```

#### UPDATE!
I realized this implementation is built without MediaPipe, which is much more complex and requires more datasets. I am going to change my plan and stick with showing landmarks and getting one hand in one window.

To capture consistently similar data for each alphabet, I added this code to keep my hand center.

```python
frameCrop = frame[y - offset: y + h + offset, x - offset: x + w + offset]

# Putting frameCrop on top of frameWhite and centering it
aspectRatio = h / w
if aspectRatio > 1:
    k = frameSize / h
    wCalculated = math.ceil(k * w)
    frameResize = cv2.resize(frameCrop, (wCalculated, frameSize))
    wGap = math.ceil((frameSize - wCalculated) / 2)
    frameWhite[:, wGap:wGap + wCalculated] = frameResize
else:
    k = frameSize / w
    hCalculated = math.ceil(k * h)
    frameResize = cv2.resize(frameCrop, (frameSize, hCalculated))
    hGap = math.ceil((frameSize - hCalculated) / 2)
    frameWhite[hGap:hGap + hCalculated, :] = frameResize
```

## Using properties written in MediaPipe, I made it work
```python
offset = 20
frameSize = 300
if allHands:
    hand = allHands[0]
    x, y, w, h = hand['bbox']
    lmList = hand['lmList']
    resized_LmList = []

    # Matrix of 300 x 300 x 3 (RGB) with uint8
    frameWhite = np.ones((frameSize, frameSize, 3), np.uint8) * 255

    # Putting frameCrop on top of frameWhite and centering it
    aspectRatio = h / w

    if aspectRatio > 1:
        k = frameSize / h
        wCalculated = math.ceil(k * w)
        wGap = math.ceil((frameSize - wCalculated) / 2)

        for lm in lmList:
            lmX = lm[0] - x + offset
            lmY = lm[1] - y + offset
            lmX = int(lmX * k) + wGap
            lmY = int(lmY * k)
            resized_LmList.append([lmX, lmY])
    else:
        k = frameSize / w
        hCalculated = math.ceil(k * h)
        hGap = math.ceil((frameSize - hCalculated) / 2)

        for lm in lmList:
            lmX = lm[0] - x + offset
            lmY = lm[1] - y + offset
            lmX = int(lmX * k)
            lmY = int(lmY * k) + hGap
            resized_LmList.append([lmX, lmY])

    pos = resized_LmList

    # Landmark connections based on MediaPipe
    landmark_connections = [
        (0, 1), (1, 2), (2, 3), (3, 4),
        (5, 6), (6, 7), (7, 8),
        (9, 10), (10, 11), (11, 12),
        (13, 14), (14, 15), (15, 16),
        (17, 18), (18, 19), (19, 20),
        (5, 9), (9, 13), (13, 17),
        (0, 5), (0, 17)
    ]

    for (node1, node2) in landmark_connections:
        cv2.line(frameWhite, 
            (pos[node1][0], pos[node1][1]), 
            (pos[node2][0], pos[node2][1]), 
            (0, 255, 0), 3
        )

    for i in range(21):
        cv2.circle(frameWhite, 
            (pos[i][0], pos[i][1]),
            4, (255, 0, 0), 1
        )

    cv2.imshow("frameWhite", frameWhite)
```

Now it only displays landmarks of hand in white background, which will require less data to make it functional.

### Seperating each image into train, valid, test sets

While trying to code to split between train, validation, and test sets, I came across an interesting post. Apparently, the fastest way to exclude a set of numbers from random.randint is:
```python
random.choice(list(set([x for x in range(0, 9)]) - set(to_exclude)))
```

## Progress Check

I managed to capture 200 images per alphabet, which is quite good progress. Before capturing the data, I was deciding between capturing the locations of hand landmarks and training through LSTM, or using CNN with images. I have decided to stick with CNN. After reading papers, I considered using ResNet but decided to use a simpler CNN instead. I also found that people in the ML industry often convert image data or any large data into hdf5 format. Therefore, I plan to learn how to convert all my PNG data into hdf5 files.

### Important Notice - Aug 3, 2024. I forgot to write more blogs on handsignalCV. Please check out my github repo for more details