---
title: 'First Kaggle Competition + Regularization'
date: 2024-06-27
permalink: /posts/2024/06/blog-post-2/
tags:
  - Kaggle
  - Regularization
  - Machine Learning Tricks
---

## 1st Attempt on Kaggle Competition
Yesterday, I tried for the first time Kaggle competition and obviously it was the Titanic one. I made some analysis based on pairplot using seaborn. I also realized in the heatmap that a lot of age data is missing. Using what I learned on Udemy, I got a median value of each PClass as older people tend to get 1 and younger get 3 to fill the missing data. 

Afterwards, I used RandomForestClassifier to get the best matching data. However, I got 78% and realized I have a long way to go. I will definitely retry next week, but now I know what to do until next weekendâ€”study more about neural networks.

```python
    # This Python 3 environment comes with many helpful analytics libraries installed
    # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
    # For example, here's several helpful packages to load
    
    import numpy as np # linear algebra
    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
    
    # Input data files are available in the read-only "../input/" directory
    # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
    
    import os
    for dirname, _, filenames in os.walk('/kaggle/input'):
        for filename in filenames:
            print(os.path.join(dirname, filename))
    
    # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
    # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
    
    train_data = pd.read_csv('/kaggle/input/titanic/train.csv')
    train_data.head()
    
    test_data.head()
    
    import seaborn as sns
    sns.pairplot(train_data)
    
    train_data.isnull()
    
    sns.heatmap(train_data.isnull(), cbar=False,cmap='coolwarm')
    
    train_data['Age'].hist(bins=30, alpha=0.7)
    
    train_data['Cabin'].hist(bins=30)
    
    train_data.groupby('Pclass')['Age'].mean()
    
    def impute_age(cols):
        Age = cols.iloc[0]
        Pclass = cols.iloc[1]
        
        if pd.isnull(Age):
            
            if Pclass == 1:
                return 38.23
            elif Pclass == 2:
                return 29.87
            else:
                return 25.14
        else:
            return Age
    
    train_data['Age'] = train_data[['Age','Pclass']].apply(impute_age,axis=1)
    sns.heatmap(train_data.isnull())
    
    train_data.drop('Cabin', axis=1, inplace=True)
    sns.heatmap(train_data.isnull())
    
    test_data = pd.read_csv("/kaggle/input/titanic/test.csv")
    test_data['Age'] = test_data[['Age','Pclass']].apply(impute_age,axis=1)
    from sklearn.ensemble import RandomForestClassifier
    
    y = train_data["Survived"]
    
    features = ["Pclass", "Sex", "SibSp", "Parch", "Age"]
    X = pd.get_dummies(train_data[features])
    X_test = pd.get_dummies(test_data[features])
    
    model = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=1)
    model.fit(X, y)
    predictions = model.predict(X_test)
    
    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})
    output.to_csv('submission.csv', index=False)
    print("Your submission was successfully saved!")
```

## Regularization

Today, I am looking over [Deep Learning Specialization by Andrew Ng](https://github.com/amanchadha/coursera-deep-learning-specialization). As they review certain elements from the machine learning specialization, it teaches other forms of the sigmoid function like np.tanh(z), which is basically a shifted version of the sigmoid function and works better for hidden units. Also, ReLU has its other derived activation function called Leaky ReLU, which is preferred when input is negative and the slope is preferred to be small.

When it comes to the difference between Deep Neural Networks and Shallow Neural Networks, Deep NN has more than 3 layers, and Shallow NN has one or two layers.

Furthermore, in the discussion between L1 and L2 regularization, I searched for the exact difference between the two and this [website](https://explained.ai/regularization/L1vsL2.html#:~:text=From%20a%20practical%20standpoint%2C%20L1,you%20have%20collinear%2Fcodependent%20features.) states that "From a practical standpoint, L1 tends to shrink coefficients to zero whereas L2 tends to shrink coefficients evenly. L1 is therefore useful for feature selection, as we can drop any variables associated with coefficients that go to zero. L2, on the other hand, is useful when you have collinear/codependent features."

Another type of regularization is Dropout Regularization which eliminates some neurons (weights) on each iteration based on a probability. From my understanding, it drops weights to create smaller NN (similar regularizing effect with L2). Furthermore, Dropout prevents overfitting. By lowering the keep-prob for some layers than others, this prevents the situation, but it provides more hyperparameters to search for using cross-validation. Another alternative that Andrew mentions is to have some layers to dropout and some layers not to, which accomplishes one hyperparameter (keep_prob for the layers where you do apply dropouts).

Other regularization methods are:
  * Data augmentation
  * Early Stopping

When Vanishing/Exploding gradients in NN happen, one of the solutions is to have more careful random initialization. For example,
```python
  np.random.rand(shape) * np.sqrt(2/n[l-1])
```
