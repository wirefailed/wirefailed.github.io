---
title: 'Building my first Computer Vision'
date: 2024-06-27
permalink: /posts/2024/06/blog-post-1/
tags:
  - HandSignalCV
  - OpenCV
  - mediapipe
---

Today marks the beginning of my journey into building a Computer Vision project after completing the Coursera ML Specialization videos. I’m new to Computer Vision (CV), so I started by watching YouTube videos on hand detection. I learned that Python often uses virtual environments to manage library versions.

## Installing Libraries

To start with Computer Vision, I need to install the OpenCV and MediaPipe libraries. While I’ve seen code examples from YouTubers, I prefer reading through the documentation to understand each function better.

* [MediaPipe on PyPI](https://pypi.org/project/mediapipe/)
* [Google Research Blog on MediaPipe](https://research.google/blog/on-device-real-time-hand-tracking-with-mediapipe/)
* [MediaPipe Resources](https://ai.google.dev/edge/mediapipe/solutions/guide)
* [Example Project on MediaPipe](https://ai.google.dev/edge/mediapipe/solutions/vision/gesture_recognizer?_gl=1*1gals79*_up*MQ..*_ga*MTY0MzUyOTAwNC4xNzE5NTI0NDQz*_ga_P1DBVKWT6V*MTcxOTUyNDQ0Mi4xLjAuMTcxOTUyNDQ0Mi4wLjAuNzI1NTM4NDk3)

## Capturing Video

For CV, I start by enabling the camera with the following code:

```python
import cv2

cap = cv2.VideoCapture(0)
while True:
    data, image = cap.read()
    cv2.imshow("Image", image)
    cv2.waitKey(1)
```

## Hand Detection with MediaPipe

I then used MediaPipe to recognize hand motions. Here’s how I did it:

```python
import cv2
import mediapipe as mp

# Drawing utilities for visualization
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
mphands = mp.solutions.hands

# Video capture from the camera
cap = cv2.VideoCapture(0)
hands = mphands.Hands(max_num_hands=1, min_detection_confidence=0.7) # Adjust confidence as needed

while True:
    _, frame = cap.read()
    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Convert BGR to RGB

    # Process the frame
    results = hands.process(framergb)

    # Draw landmarks on the frame
    if results.multi_hand_landmarks:
        for handlms in results.multi_hand_landmarks:
            mp_drawing.draw_landmarks(
                frame,
                handlms, 
                mphands.HAND_CONNECTIONS,
                mp_drawing_styles.get_default_hand_landmarks_style(),
                mp_drawing_styles.get_default_hand_connections_style()
            )

    cv2.imshow('Handtracker', frame)
    if cv2.waitKey(1) & 0xFF == 27:  # Press ESC to exit
        break

cap.release()
cv2.destroyAllWindows()
```

## Next Steps

I was planning to integrate a pretrained model from a YouTube video, but they use their own version of python library that integrates both OpenCV and MediaPipe. Therefore, I will examine their GitHub repository and MediaPipe to understand their code.

* [CVZone Github Repo](https://github.com/cvzone/cvzone/tree/master/cvzone)
* [Hand Landmark Model Documentation](https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/hands.md)

## Collect Data

To create my project, I need to collect data using the [imwrite function](https://www.geeksforgeeks.org/python-opencv-cv2-imwrite-method/). Additionally, I am exploring ways to crop detected hands and display them as separate images. I’m also considering other options, such as printing only the landmarks instead of capturing pictures of my hand.
